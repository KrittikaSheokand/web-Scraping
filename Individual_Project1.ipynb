{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Scraping and Saving HTML Content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Navigated to https://sfbay.craigslist.org/search/zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2:  Interact with the Page-Sorting:\n",
    "URL when the page is sorted - newest: https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~0~0\n",
    "URL when the page is sorted - oldest: https://sfbay.craigslist.org/search/zip?sort=dateoldest#search=1~gallery~0~0\n",
    "\n",
    "Changing the sorting criteria changes the URL - sort=date for newest whereas sort=dateoldest for oldest\n",
    "Sorting area can be changed by modifying the sort='date' or 'dateoldest in URL in browser's address bar\n",
    "\n",
    "Changing the sort order is a GET request. It's part of the URL and is not hidden.\n",
    "\n",
    "And the variable associated is sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: \n",
    "URL for page 1 : https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~0~0\n",
    "\n",
    "URL for page 2 : https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~1~0\n",
    "\n",
    "URL for page 3 : https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~2~0\n",
    "\n",
    "Only one number changes when you move pages on the URL which is the number next to 'gallery~'. The number is one less t3:han the page number. For instance, page 1 has 0 next to 'gallery~' in URL and page 2 has 1 there. This number can be changed accordingly to navigate between pages directly from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "# headers = {'User-agent': 'Mozilla/5.0'}\n",
    "\n",
    "url = 'https://sfbay.craigslist.org/search/zip?sort=date#search=1~gallery~0~0'\n",
    "page = requests.get(url) #headers=headers)\n",
    "\n",
    "print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Look at the unmodified source.\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 'ol' element with class 'cl-static-search-results' which has search results and 'a' element within has link to the item. The code for the same is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ol = soup.select_one(\"ol.cl-static-search-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ol_a = page_ol.select('a')\n",
    "print(page_ol_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can directly search for a however, here you start getting from third index. First two aren't item links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_a = soup.select(\"a\")\n",
    "print(page_a[3].get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to first method and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how links we got\n",
    "print(len(page_ol_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I got more than 250 links here, I'll pull first 250 using a loop and make a list of the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store first 250 links in a list\n",
    "url_list = []\n",
    "for i in range(0, 250):\n",
    "    url_list.append(page_ol_a[i].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save HTML Pages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveString(html, filename=\"test.html\"):\n",
    "\ttry:\n",
    "\t\tfile = open(filename, \"w\", encoding='utf-8')\n",
    "\t\tfile.write(str(html))\n",
    "\t\tfile.close()\n",
    "\texcept Exception as ex:\n",
    "\t\tprint('Error: ' + str(ex))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in url_list:\n",
    "    time.sleep(5)\n",
    "    print(url)\n",
    "    page = requests.get(url) \n",
    "    match = re.search(r'/(\\d+)\\.html$', url)\n",
    "    id = match.group(1)\n",
    "    filename = f\"{id}.html\"\n",
    "    saveString(page.content,filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Parsing and Displaying Information from Saved HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Saved HTML Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file in the directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "#     # Check if the file ends with .html\n",
    "\n",
    "    if filename.endswith(\".html\"):\n",
    "        # print(filename)\n",
    "\n",
    "#         # Construct the full file path\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # print(filepath)\n",
    "\n",
    "#         # Read file to string\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "\n",
    "                html = file.read()\n",
    "\n",
    "#         # ... use html string ...\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                # print(soup.prettify())\n",
    "                #Get title\n",
    "                title_1 = soup.select(\"span#titletextonly\")\n",
    "                for i in title_1:\n",
    "                        print(title_1[0].text)\n",
    "                #Get image source\n",
    "                img_1 = soup.select(\"div.gallery\")\n",
    "                img_2 = img_1.select(\"img\")\n",
    "                print(img_2['src'])\n",
    "                #Get description\n",
    "                desc = soup.select(\"#postingbody\")\n",
    "                for i in desc:\n",
    "                        print(i.text)\n",
    "                # #Get post id\n",
    "                post_id_1 = soup.select(\"p.postinginfo\")\n",
    "                post_id = post_id_1[1]\n",
    "                print(post_id_1[1].text)\n",
    "                #get post date\n",
    "                #it exists in first time.date timeago\n",
    "                posted_date = soup.select(\"time.date.timeago\")\n",
    "                print(posted_date[0].title)\n",
    "                # #Get update info (If it exists, it exists in second time.date timeago )\n",
    "                update_info = soup.select('time.date.timeago')\n",
    "                print(posted_date[1].title)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Automating Login on The Old Reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made an account on https://theoldreader.com/\n",
    "\n",
    "Using browser’s developer tools to inspect the page and focusing on the <form> tag involved in the login process - `<input>` fields within the login form are utf-8, authenticity_token, user[login], user[password] and commit\n",
    "\n",
    " \n",
    "\n",
    "Looking at the network tab while logging in, it can be seen that POST request was made while logging in. This is to keep login information hidden.\n",
    "Payload has following items:\n",
    "utf-8 - ✓\n",
    "authenticity_token: bmdF4HMov351nBfIZ8R9SbQQUxj6boJ9gGVP10+LEUA=\n",
    "user[login]\n",
    "user[password]\n",
    "commit: Sign In\n",
    "\n",
    "The input elements are the same as seen on html elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'Mozilla/5.0'}\n",
    "\n",
    "url = 'https://theoldreader.com/users/sign_in'\n",
    "page = requests.get(url, headers=headers)\n",
    "# print(page)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = soup.select_one('#new_user input[name=authenticity_token]')\n",
    "authenticity_token = input.get('value')\n",
    "# print(authenticity_token)\n",
    "\n",
    "time.sleep(5) # 5s\n",
    "\n",
    "\n",
    "# A session to carry the cookies and make post requests\n",
    "session = requests.session()\n",
    "\n",
    "res = session.post('https://theoldreader.com/users/sign_in', \n",
    "                        data = {'utf8' : '✓',\n",
    "                                'authenticity_token' : authenticity_token,\n",
    "                                'user[login]' : '***', \n",
    "                                'user[password]' : '***',\n",
    "                                'commit' : 'Sign In'}, \n",
    "                        timeout = 20)\n",
    "\n",
    "# Get the Cookies.\n",
    "cookies = session.cookies.get_dict()\n",
    "print(cookies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session stores session information, user name, a token to remember user and signed at cookies to identify user, and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # 5s\n",
    "\n",
    "# To remain in-session.\n",
    "page_n = requests.get('https://theoldreader.com', cookies=cookies) # or explicitly set cookies of the session (when not using session.xyz)\n",
    "soup_n = BeautifulSoup(page_n.content, 'html.parser')\n",
    "print(soup_n.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the page, you can see\n",
    "Bugsnag.user = {\n",
    "      id: \"******\",\n",
    "      name: \"******\",\n",
    "      email: \"******\"\n",
    "    }\n",
    "\n",
    "which is my email and name - identified by cookies which means I was able to log in successfully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
